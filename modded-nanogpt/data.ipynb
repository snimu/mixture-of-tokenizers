{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "04be6e2d-2f4a-45b1-b322-86b68e08d266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tiktoken\n",
    "import os\n",
    "from pathlib import Path\n",
    "import torch\n",
    "from typing import Literal\n",
    "import json\n",
    "from time import perf_counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "88eada96-9b05-4544-8bd0-fb223cd7aee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = tiktoken.encoding_for_model(\"gpt2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "18690728-0be8-436a-ba48-e257962a08af",
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_size = 50257"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "e2d00db6-4696-49b6-9729-d00d60746a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "decoded_tokens = [enc.decode([idx]) for idx in range(vocab_size)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e6ab26f-8f2d-453e-a691-60f2f27eaae4",
   "metadata": {},
   "source": [
    "## Token lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "d47b394c-55c1-44b0-98d4-7d1c6cb62704",
   "metadata": {},
   "outputs": [],
   "source": [
    "lengths = torch.tensor([len(t) for t in decoded_tokens], dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "94734664-e375-4b74-a02c-78eb777bb9d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(6.3544), tensor(66.))"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lengths.mean(), lengths.max()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a4c8642-0b37-46c0-a3fc-af372b297ae8",
   "metadata": {},
   "source": [
    "Apparently, the longest tokens has 66 bytes. That's way too much; if possible, I'll just use the first / last n bytes and hope that the tokenization itself will take care of this. In the future, just don't design stupid tokenizers like this.\n",
    "\n",
    "For now, I'll try to find out a sensible cutoff length.\n",
    "\n",
    "First off, define some helpers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "d2a6f0fa-9d80-41ee-b1d8-079e9cff844d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def with_gt_n_bytes(n: int) -> list[int]:\n",
    "    return torch.where(lengths >= n)[0].tolist()\n",
    "\n",
    "def decode_tokens(tokens: list[int]) -> list[str]:\n",
    "    return [enc.decode([t]) for t in tokens]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ebc1ee2-dc0b-41e8-ae27-0a90379b82bd",
   "metadata": {},
   "source": [
    "### Count tokens\n",
    "\n",
    "How many are there with more than n bytes, for n in [16, 66]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "id": "edda5eed-af16-4010-bce2-d3f899775e83",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{16: 121,\n",
       " 17: 61,\n",
       " 18: 45,\n",
       " 19: 34,\n",
       " 20: 29,\n",
       " 21: 28,\n",
       " 22: 25,\n",
       " 23: 25,\n",
       " 24: 24,\n",
       " 25: 21,\n",
       " 26: 21,\n",
       " 27: 21,\n",
       " 28: 21,\n",
       " 29: 21,\n",
       " 30: 21,\n",
       " 31: 21,\n",
       " 32: 21,\n",
       " 33: 12,\n",
       " 34: 10,\n",
       " 35: 9,\n",
       " 36: 9,\n",
       " 37: 9,\n",
       " 38: 9,\n",
       " 39: 9,\n",
       " 40: 9,\n",
       " 41: 9,\n",
       " 42: 9,\n",
       " 43: 9,\n",
       " 44: 9,\n",
       " 45: 9,\n",
       " 46: 9,\n",
       " 47: 9,\n",
       " 48: 9,\n",
       " 49: 8,\n",
       " 50: 8,\n",
       " 51: 8,\n",
       " 52: 8,\n",
       " 53: 8,\n",
       " 54: 8,\n",
       " 55: 8,\n",
       " 56: 8,\n",
       " 57: 7,\n",
       " 58: 7,\n",
       " 59: 7,\n",
       " 60: 7,\n",
       " 61: 7,\n",
       " 62: 7,\n",
       " 63: 7,\n",
       " 64: 7,\n",
       " 65: 2,\n",
       " 66: 1}"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "hist = dict()\n",
    "for num in range(16, 67):\n",
    "    hist[num] = len(with_gt_n_bytes(num))\n",
    "hist"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74e98869-4730-4998-9588-6af77ed4fb13",
   "metadata": {},
   "source": [
    "### Where to make the cutoff\n",
    "\n",
    "Are threre any relevant tokens with more than n bytes?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0564a7f2-447c-43bf-9e0b-161e71b3a888",
   "metadata": {},
   "source": [
    "#### 16 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "db3c9460-50d3-4ae6-81d5-51499368896f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' incomprehensible',\n",
       " ' technologically',\n",
       " ' Telecommunications',\n",
       " '..................',\n",
       " 'oooooooooooooooo',\n",
       " ' Congratulations',\n",
       " ' inappropriately',\n",
       " '////////////////////////////////']"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_tokens(with_gt_n_bytes(16)[-8:])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a86dab42-d950-4c5f-ae25-cf06db745784",
   "metadata": {},
   "source": [
    "**CONCLUSION** &mdash; ' incomprehensible' is an important token &rarr; I cannot go below 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a136b0b0-4d57-49a7-80f7-de6d9d613702",
   "metadata": {},
   "source": [
    "#### 18 bytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "id": "d1f01ebe-8b45-4f2f-b65d-82b5149f5a0b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--------------------------------',\n",
       " '................................',\n",
       " '================================',\n",
       " '----------------------------------------------------------------',\n",
       " '________________________________',\n",
       " ' ----------------------------------------------------------------',\n",
       " '********************************',\n",
       " '--------------------',\n",
       " ' --------------------------------',\n",
       " '------------------------',\n",
       " 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ',\n",
       " '................................................................',\n",
       " '================================================================',\n",
       " '________________________________________________________________',\n",
       " ' telecommunications',\n",
       " '........................',\n",
       " ' disproportionately',\n",
       " '################################',\n",
       " ' guiActiveUnfocused',\n",
       " ' externalToEVAOnly',\n",
       " 'cloneembedreportprint',\n",
       " 'rawdownloadcloneembedreportprint',\n",
       " 'externalActionCode',\n",
       " '________________________',\n",
       " 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ',\n",
       " ' RandomRedditorWithNo',\n",
       " ' =================',\n",
       " ' =================================================================',\n",
       " 'ItemThumbnailImage',\n",
       " 'quickShipAvailable',\n",
       " 'isSpecialOrderable',\n",
       " 'channelAvailability',\n",
       " 'BuyableInstoreAndOnline',\n",
       " ' environmentalists',\n",
       " ' --------------------',\n",
       " ' ********************************',\n",
       " ' SolidGoldMagikarp',\n",
       " ' indistinguishable',\n",
       " '--------------------------------------------------------',\n",
       " ' =================================',\n",
       " ' counterproductive',\n",
       " '------------------------------------------------',\n",
       " ' Telecommunications',\n",
       " '..................',\n",
       " '////////////////////////////////']"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_tokens(with_gt_n_bytes(18))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d3fa8ae-97d6-43e2-b793-5b2b3ab4ac13",
   "metadata": {},
   "source": [
    "**CONCLUSION** &mdash; some potentially relevant tokens in here."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "447a89e9-fe42-4c1e-80db-ca7edf24a4df",
   "metadata": {},
   "source": [
    "#### 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "057383ec-d810-429c-b14b-c15667e9245e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['--------------------------------',\n",
       " '................................',\n",
       " '================================',\n",
       " '----------------------------------------------------------------',\n",
       " '________________________________',\n",
       " ' ----------------------------------------------------------------',\n",
       " '********************************',\n",
       " '--------------------',\n",
       " ' --------------------------------',\n",
       " '------------------------',\n",
       " 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ',\n",
       " '................................................................',\n",
       " '================================================================',\n",
       " '________________________________________________________________',\n",
       " '........................',\n",
       " '################################',\n",
       " 'cloneembedreportprint',\n",
       " 'rawdownloadcloneembedreportprint',\n",
       " '________________________',\n",
       " 'ÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂÃÂ',\n",
       " ' RandomRedditorWithNo',\n",
       " ' =================================================================',\n",
       " 'BuyableInstoreAndOnline',\n",
       " ' --------------------',\n",
       " ' ********************************',\n",
       " '--------------------------------------------------------',\n",
       " ' =================================',\n",
       " '------------------------------------------------',\n",
       " '////////////////////////////////']"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decode_tokens(with_gt_n_bytes(20))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76dccadd-c8a7-4a20-9ca6-c4b0722828fd",
   "metadata": {},
   "source": [
    "**CONCLUSION** &mdash; 20 tokens seems like a fine cutoff, only bullshit beyond that."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b40e3cef-9967-4126-a3ec-022a06046ac0",
   "metadata": {},
   "source": [
    "## Convert tokens to bytes\n",
    "\n",
    "Convert tokens to bytes, depending on how many bytes I want to represent a single token."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "b54c9627-bedf-4ec6-89b7-a8fd2da8c84c",
   "metadata": {},
   "outputs": [],
   "source": [
    "allbytes = set()\n",
    "for token in decoded_tokens:\n",
    "    for char in token:\n",
    "        allbytes.add(str(char))\n",
    "allbytes = sorted(list(allbytes))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "4501209a-739a-4312-916c-10680135f6be",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "456"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(allbytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "3b11b72e-063a-427a-b09b-818f2dace5b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "int_to_byte = {i: char for i, char in enumerate(allbytes)}\n",
    "byte_to_int = {v:k for k, v in int_to_byte.items()}\n",
    "byte_to_int[\"pad\"] = len(allbytes)\n",
    "byte_to_int[\"endoftext\"] = len(allbytes)+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "b445b13f-f5ce-4f03-90f2-ab94910c1dfa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _token_to_bytes_right_aligned(token: str, num_bytes: int) -> list[int]:\n",
    "    if token == enc.decode([vocab_size-1]):\n",
    "        return [byte_to_int[\"endoftext\"]] * num_bytes\n",
    "    if len(token) > num_bytes:\n",
    "        return [byte_to_int[char] for char in token[-num_bytes:]]\n",
    "\n",
    "    padded = [byte_to_int[\"pad\"]] * num_bytes\n",
    "    decoded = [byte_to_int[char] for char in token]\n",
    "    for i, num in enumerate(reversed(decoded)):\n",
    "        padded[-i-1] = num\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def _token_to_bytes_left_aligned(token: str, num_bytes: int) -> list[int]:\n",
    "    if token == enc.decode([vocab_size-1]):\n",
    "        return [byte_to_int[\"endoftext\"]] * num_bytes\n",
    "    if len(token) > num_bytes:\n",
    "        return [byte_to_int[char] for char in token[:num_bytes]]\n",
    "\n",
    "    padded = [byte_to_int[\"pad\"]] * num_bytes\n",
    "    for i, char in enumerate(token):\n",
    "        padded[i] = byte_to_int[char]\n",
    "\n",
    "    return padded\n",
    "\n",
    "\n",
    "def token_to_bytes(token: str, num_bytes: int, alignment: Literal[\"left\", \"right\"]) -> list[int]:\n",
    "    assert alignment in (\"left\", \"right\")\n",
    "    if alignment == \"left\":\n",
    "        return _token_to_bytes_left_aligned(token, num_bytes)\n",
    "    else:\n",
    "        return _token_to_bytes_right_aligned(token, num_bytes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "865da55c-a785-4bf1-b609-4f9a15969d2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([97, 98, 99, 100, 456, 456], [456, 456, 97, 98, 99, 100])"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "token_to_bytes(\"abcd\", 6, \"left\"), token_to_bytes(\"abcd\", 6, \"right\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a003fe4c-c1b5-4893-883a-304c268ea89b",
   "metadata": {},
   "source": [
    "### Do the actual conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "id": "ab0dfae3-90f4-451e-8496-da247a953e5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "for num_bytes in (16, 18, 20):\n",
    "    for alignment in (\"left\", \"right\"):\n",
    "        ttb = dict()\n",
    "        for idx in range(vocab_size):\n",
    "            ttb[idx] = token_to_bytes(token=enc.decode([idx]), num_bytes=num_bytes, alignment=alignment)\n",
    "        with open(f\"ttb_{num_bytes}_{alignment}.json\", \"w\") as f:\n",
    "            f.write(json.dumps(ttb))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3eb34ae6-d083-40c8-942f-8eaee75b528f",
   "metadata": {},
   "source": [
    "## How do I change the dataloader with this in mind?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "867dd5cb-89e2-46ce-83aa-2ad3f74d17f9",
   "metadata": {},
   "source": [
    "### Baseline\n",
    "\n",
    "Take the dataloader from modded-nanogpt as a baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "cf3f8b00-5acd-4802-8ee4-8c6bd2f69a32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _load_data_shard(file: Path):\n",
    "    header = torch.from_file(str(file), False, 256, dtype=torch.int32) # header is 256 int32\n",
    "    assert header[0] == 20240520, \"magic number mismatch in the data .bin file\"\n",
    "    assert header[1] == 1, \"unsupported version\"\n",
    "    num_tokens = int(header[2]) # number of tokens (claimed)\n",
    "    with file.open(\"rb\", buffering=0) as f:\n",
    "        tokens = torch.empty(num_tokens, dtype=torch.uint16, pin_memory=True) # avoid pin_memory copy by @YouJiacheng\n",
    "        f.seek(256 * 4)\n",
    "        nbytes = f.readinto(tokens.numpy()) # avoid bytes->array copy by @YouJiacheng\n",
    "        assert nbytes == 2 * num_tokens, \"number of tokens read does not match header\"\n",
    "    return tokens\n",
    "\n",
    "def distributed_data_generator(filename_pattern: str, batch_size: int, rank : int, world_size : int):\n",
    "    files = sorted(Path.cwd().glob(filename_pattern))\n",
    "    assert batch_size % world_size == 0\n",
    "    local_batch_size = batch_size // world_size\n",
    "    file_iter = iter(files) # use itertools.cycle(files) instead if you want to do multi-epoch training\n",
    "    tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "    while True:\n",
    "        if pos + batch_size + 1 >= len(tokens):\n",
    "            tokens, pos = _load_data_shard(next(file_iter)), 0\n",
    "        buf = tokens[pos + rank * local_batch_size:][:local_batch_size + 1]\n",
    "        inputs = buf[:-1].to(device=\"cuda\", dtype=torch.int32, non_blocking=True) # no sync on host side;\n",
    "        targets = buf[1:].to(device=\"cuda\", dtype=torch.int64, non_blocking=True) # H2D in another stream isn't helpful.\n",
    "        pos += batch_size\n",
    "        yield inputs, targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "73dbc717-9171-4ab4-a88f-505a1373cd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "54.016402913599904"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_tries = 10\n",
    "times = list()\n",
    "for _ in range(num_tries):\n",
    "    t0 = perf_counter()\n",
    "    ddg = distributed_data_generator(\"data/fineweb10B/fineweb_train_*.bin\", 1024, 1, 1)\n",
    "    try:\n",
    "        for inputs, targets in ddg:\n",
    "            pass\n",
    "    except Exception:\n",
    "        pass\n",
    "    times.append(perf_counter() - t0)\n",
    "\n",
    "t = sum(times) / len(times)\n",
    "t"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "857f6fdb-89ed-4f45-a334-e8eee02442d2",
   "metadata": {},
   "source": [
    "### Can I just do the conversion online?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
